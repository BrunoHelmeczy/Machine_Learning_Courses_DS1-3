---
title: "Regressions, PCA, Clustering"
author: "Bruno Helmeczy"
date: "21/02/2021"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
  pdf_document: default
---
This report summarizes my homework made for the Data Science 1: Machine Learning concepts class at the Central European University's MSc Business Analytics program. The 1st Chapter looks to predict property valuations in Manhattan, New York, using with OLS & Penalized models: LASSO, Ridge, & Elastic Net regression. 

The chapter also extends to finding the "simplest still good enough" model specifications, and executing said models using the datas' principle components, after preProcessing the dataset via Principal Component Analysis. The final model chosen for Chapter 1 is the best Elastic Net model, with RMSE 0.5, MAE 0.38 & R-squared of 89.68% performance during 10-fold Cross-Validation. Tested on the holdout set (70% of all observations), the model remained stable, indicating robust expected performance. 

The 2nd Chapter looks to cluster US states, using the USArrests dataset, after a brief principal component analysis. I first found the optimal number of clusters being 2, then calculated conditional column means to find clusters' defining characteristics. Plotting the data along the 1st 2 principal components shows a distinct seperation, while the final visualizations shows weights used to calculate either Principal Component. One can see, based on grouped means table & weights used, that principal component 1 is most heavily based on states' crime-related statistic, while principal component 2 on population size. Plotting the cluster-colored data along these PCs shows a clear line of seperation at ca. PC1 = 0.    




```{r setup, message = F, warning = F, echo = T}
library(tidyverse)
library(datasets)
library(MASS)
library(ISLR)
library(caret)
library(data.table)
library(ggthemes)
library(ggridges)
library(gridExtra)
library(knitr)
library(kableExtra)
library(factoextra) # provides nice functions for visualizing the output of PCA
library(NbClust) # for choosing the optimal number of clusters

rm(list=ls())
options(scipen=999)
theme_set(theme_tufte())

df <- readRDS(url('http://www.jaredlander.com/data/manhattan_Train.rds')) %>%
  mutate(logTotalValue = log(TotalValue)) %>%
  drop_na() %>% dplyr::select(-TotalValue)

```


### 1) Supervised Learning with Penalized Models & PCA

As said, the 1st chapter looks to predict Manhattan properties' total valuations, specifically logTotalValue, based on an array of features, including various zoning districts, health or police area codes, and size of special-purpose areas within a building, e.g. Commercial / Garage area. 

As part of cleaning my data, my 1st helper function helps me keep an overview, by returning column names, classes & number of distinct values - possible to call anytime, thus useful as a status checkup, or returning the current column names to loop through in later functions. I also delete Zoning Districts 2-4 due to 60%+ missing values in each, transform Yes / No & Logical variables to 1s & 0s, and specify Council district, Health Area Code & Police Prct as categorical variables, i.e. factors. I do so based on the data description available on github [(here)](https://github.com/NYCPlanning/db-pluto/blob/master/metadata/Data_Dictionary.md?fbclid=IwAR0lNCMLS6ySPi-eXCoVyNbePj9klTA9KYM91HJ36B3YeQt1l4-9EbIqG3s).


```{r helpers, message = F, warning = F, echo = T}
### 1) Supervised Learning with Penalized Models & PCA ----
    # Property Values in Manhattan - R 4 Everyone
    #   Predict log-Property Value = logTotalValue

# Function to check distinct values & class of all columns - anytime
ColsUniques <- function(dataframe = df) {
  rbindlist(lapply(1:length(dataframe), function(x) {
    tl <- list()
    tl[['name']] <- colnames(dataframe)[x]
    tl[['distinct']] <- nrow(unique(dataframe[,x]))
    tl[['class']] <- dataframe[,x][[1]] %>% class()
    return(tl)
  }))  
}
ColsUni1 <- ColsUniques()

    # Zone Distr 2-4 -> 90+ %  Missing value
df$ZoneDist4 <- NULL
df$ZoneDist3 <- NULL
df$ZoneDist2 <- NULL

# Binary Factors -> Yes = 1 No = 0
df <- df %>% mutate(
  IrregularLot     = ifelse(IrregularLot == "Yes",1,0),
  Landmark         = ifelse(Landmark == "Yes",1,0),
  HistoricDistrict = ifelse(HistoricDistrict == "Yes" ,1,0),
  High             = ifelse(High == T,1,0)
) %>% mutate(
  Council    = as.factor(Council),
  PolicePrct = as.factor(PolicePrct),
  HealthArea = as.factor(HealthArea)
)

ColsUni2 <- ColsUniques()
```


#### 1.a) Short EDA on data -> Find possible predictors 

As a 1st step I explored variables' frequency distributions, and association patterns with the target variable. Given the number of variables & that I wanted to re-check association patterns after possible transformations, I wrote 2 functions to iterate through all columns and plot each, using the ColsUniques() function defined above. For the 2nd function, Boxes_n_Scatters() I use boxplots when the X variable used is categorical or binary, and a scatter plot plus a LOESS curve when numeric. Thus, I found most area- or size-related variables to be skewed with a long right-tail, so log-transformed them, while creating dummy variables to indicate 0s in these variables, meaning e.g. in case of Garage area, that a property does not have a garage. For log-transformations, I added an insignificant amount to avoid infinite values, & finally I chose to only plot top correlations with the target variable. 

```{r a , message = F, warning = F, echo = T, fig.align= 'center', fig.height=3,fig.width=6}

# 2 functions to check all histograms / boxplots / LOESS-es in dataset
Hists <- function() {
  ColsUni2 <- ColsUniques()
  
  lapply(df %>% dplyr::select(-matches("id")) %>% colnames(),function(x) {
    if( ( ColsUni2$class[ColsUni2$name == x] %in% c("integer","numeric") ) & (df[,x] %>% unique() %>% nrow() >2 )  ) {
    plot <- df %>% ggplot(aes_string(x = x)) + 
        geom_histogram( color = "red", fill = "blue") + theme_tufte() + 
        labs(title = paste0("NYC Properties ",x," Distribution")) 
    print(plot)
    } else {
      plot <- df %>% ggplot(aes_string(x = x ) ) + 
        geom_bar(color = "red", fill = "blue") + coord_flip() + theme_tufte() + 
        labs(title = paste0("NYC Properties ",x," Distribution"))  
      print(plot)
    }
  })
  
}

Boxes_n_Scatters <- function() {
  ColsUni1 <- ColsUniques()
  
# Box Plots
  lapply(df %>% dplyr::select(-matches("id|logTotalValue")) %>%  colnames(), function(x) {
      if (  (ColsUni1$class[ColsUni1$name == x] %in% c("factor","logical") ) | (df[,x] %>% unique() %>% nrow() == 2)  ) {
    plot <- df %>% ggplot()  +
          geom_boxplot(aes_string(y = x, x = "logTotalValue", group = x),color = "red", fill = "blue") +
          theme_tufte() +  
          labs(title = paste0("Title ",x))
    print(plot)
    print(paste0("Printing plot: ",x))
      } 
    })
  
# Scatters 
  xvars <- df %>% dplyr::select(
    matches(paste(ColsUni1$name[ColsUni1$class %in% 
                                  c("numeric","integer")],collapse = "|"))) %>%
    dplyr::select(-matches("id|logTotalValue")) %>% colnames()
  
  for (i in xvars) {
    plot <- df[,c(i,"logTotalValue")] %>% 
      ggplot(aes_string(x = i, y = "logTotalValue")) +
      geom_smooth() + geom_point(size = 0.1)  +
      labs(title = paste0("Title ",i))
    
    print(plot)  
    print(paste0("Printing plot: ",i))
  }
}

df <- df %>% mutate(
  BuiltFAR_Zero   = ifelse(BuiltFAR == 0, 1, 0),
  BldgDepth_Zero  = ifelse(BldgDepth == 0,1,0),
  BldgFront_Zero  = ifelse(BldgFront == 0 , 1,0),
  LotDepth_Zero   = ifelse(LotDepth == 0,1,0),
  LotFront_Zero   = ifelse(LotFront == 0,1,0),
  UnitsTotal_Zero = ifelse(UnitsTotal == 0,1,0),
  UnitsRes_Zero   = ifelse(UnitsRes == 0,1,0),
  NumFloors_Zero  = ifelse(NumFloors == 0,1,0),
  NumBldgs_Zero   = ifelse(NumBldgs == 0,1,0),
  OtherArea_Zero  = ifelse(OtherArea == 0,1,0),
  FactryArea_Zero = ifelse(FactryArea == 0,1,0),
  StrgeArea_Zero  = ifelse(StrgeArea == 0,1,0),
  GarageArea_Zero = ifelse(GarageArea == 0,1,0),
  RetailArea_Zero = ifelse(RetailArea == 0,1,0),
  OfficeArea_Zero = ifelse(OfficeArea == 0,1,0),
  ResArea_Zero    = ifelse(ResArea == 0,1,0),
  ComArea_Zero    = ifelse(ComArea == 0,1,0),
  BldgArea_Zero   = ifelse(BldgArea == 0,1,0),
  LotArea_Zero    = ifelse(LotArea == 0,1,0),
  Easements       = ifelse(Easements > 0,1,0)
)

# log-transforms: ----
# BuiltFAR, BldgDepth, BldgFront, LotDepth, LotFront, UnitTotal, UnitsRes,
# NumFloors, NumBldgs, OtherArea, FactryArea, StrgeArea, GarageArea, RetailArea,
# OfficeArea, ResArea, ComArea, BldgArea, LotArea, Easements


df <- df %>% mutate(
  BuiltFAR_ln   = log(BuiltFAR    + 0.01),
  BldgDepth_ln  = log(BldgDepth   + 0.01),
  BldgFront_ln  = log(BldgFront   + 0.01),
  LotDepth_ln   = log(LotDepth    + 0.01),
  LotFront_ln   = log(LotFront    + 0.01),
  UnitsTotal_ln = log(UnitsTotal  + 0.01),
  UnitsRes_ln   = log(UnitsRes    + 0.01),
  NumFloors_ln  = log(NumFloors   + 0.01),
  NumBldgs_ln   = log(NumBldgs    + 0.001),
  OtherArea_ln  = log(OtherArea   + 0.01),
  FactryArea_ln = log(FactryArea  + 0.01),
  StrgeArea_ln  = log(StrgeArea   + 0.01),
  GarageArea_ln = log(GarageArea  + 0.01),
  RetailArea_ln = log(RetailArea  + 0.01),
  OfficeArea_ln = log(OfficeArea  + 0.01),
  ResArea_ln    = log(ResArea     + 0.01),
  ComArea_ln    = log(ComArea     + 0.01),
  BldgArea_ln   = log(BldgArea    + 0.01),
  LotArea_ln    = log(LotArea     + 0.01)) %>% 
  dplyr::select(-c(BuiltFAR, BldgDepth, BldgFront, LotDepth, LotFront, UnitsTotal,
            UnitsRes, NumFloors, NumBldgs, OtherArea, FactryArea, StrgeArea,
            GarageArea, RetailArea, OfficeArea, ResArea, ComArea, BldgArea, LotArea))

# Correls w logTotalValue
Cols <- ColsUniques()

Y_Cors <- df %>% dplyr::select(matches(Cols$name[Cols$class %in% c("numeric", "integer")])) %>% 
  dplyr::select(-matches("id|logTotalValue")) %>% colnames() %>% 
  lapply(function(x) {
    tl <- list()
    tl[['Colname']] <- x
    tl[['Corr_w_Y_abs']] <- cor(df[,x],df[,"logTotalValue"]) %>% round(2) %>% abs()
    return(tl)
  }) %>% rbindlist() %>% as.data.frame() %>% arrange(desc(Corr_w_Y_abs))


TopCors <- Y_Cors[1:15,] %>% ggplot(aes(x = reorder(Colname, Corr_w_Y_abs), y = Corr_w_Y_abs,
                      color = "red")) +
  geom_point(size = 8) + 
  geom_col(width = 0.1, position = "dodge", fill  = "red") +
  geom_text(aes(label = Corr_w_Y_abs), size = 3, color = "black") + 
  theme_tufte() + 
  theme(legend.position = c(2,2),legend.title = element_text(size = 8)
        ,legend.text = element_text(size = 8),legend.key.size = unit(2,"mm")) +
  coord_flip() +
  labs(title = "Top Correlations with Target-Y Variable",
       y = "Absolute Correlation",
       x = "Continuous X variables")
  
# Top 3 correlating variables are LotArea, LotFront & Commercial Area, 
  # w Number of Units in a Building & total Building area making the top 5

TopCors
```

From the plot above, 1 can see that the 2 strongest correlating binary-variables with logTotalValue are whether we are talking about a high-rise building, and whether the building has commercial area. In terms of continuous variables, total land area, land area facing streets, commercial area, number of total units, & building area show the strongest log-linear association pattern, indicating possibly strong predictors.

#### 1.b) Create Training - Test sets -> 70% test - 30% training 

```{r b , message = F, warning = F, echo = T}

#### Sample vs Holdout sets
set.seed(1234)
train_indices <- as.integer(createDataPartition(df$logTotalValue, p = 0.3, list = FALSE))
data_train <- df[train_indices, ]
data_holdout <- df[-train_indices, ]

# train control is 10 fold cross validation
train_control <- trainControl(method = "cv",number = 10,verboseIter = FALSE)  


```

The exercise indicated 70% test to be used. I use createDataPartition() from caret to get row indices, & use them to split the dataset to train- & holdout samples. I also define my base train_control parameters, i.e. 10-fold cross-validation.  


#### 1 c) Lin. Regr. 2 predict logTotalValue -> 10-fold CV to assess predictive power

Below I test an OLS linear regression model, to be used as baseline vs following models below. To fully-define my feature space, I wrote a double-loop to interact all area-related variables, yielding (10 * 9)/2 = 45 interactions in total. I paste these together with all variable names except ID & logTotalValue to specify my formula used,  run the model using train(), & summarize cross-validated performance in the table below. 

```{r c , message = F, warning = F, echo = T}

# We can also expect that a garages value increases, 
    # given how large the building is, how much commercial area is there, etc.
  #   all-in-all I hypothesize a positive feedback loop between all area related variables
    #   I model this via interactions between all area related variables 
        #   -> ( 10 * 9 ) / 2 = 45 interactions 

Vars <- df %>% dplyr::select(matches("area_ln")) %>% colnames()

Interactions <- NULL
Counter <- 1
for (i in 1:length(Vars)) {
  for (j in (i+1):(length(Vars)) ) {
    Interactions[Counter] <- paste(Vars[i],Vars[j],sep =  " * ")
    Counter <- Counter + 1
  }
}
Interactions <- paste(Interactions[1:45],collapse =  " + " ) 

Vars <- df %>% dplyr::select(-matches("id|logTotalValue")) %>% colnames()
Formula <- formula(paste0("logTotalValue ~",paste(Vars, collapse = " + ")," + ", Interactions))

#### OLS ####
set.seed(1234)
ols_model <- train(
  formula(Formula),
  data = data_train,
  method = "lm",
  trControl = train_control)

CV_RMSE <- ols_model$resample[,1] %>% mean()
CV_MAE <- ols_model$resample[,3] %>% mean()
CV_R2 <- ols_model$resample[,2] %>% mean()

data_holdout$ols_pred <- predict(ols_model, newdata = data_holdout)
# Holdout
Hd_MAE <- MAE(data_holdout$ols_pred, data_holdout$logTotalValue)
Hd_RMSE <- RMSE(data_holdout$ols_pred, data_holdout$logTotalValue)

# OLS Summary - Cross-Validation & Holdout data
OLS_Summ <- as.data.frame(cbind(CV_MAE,CV_RMSE,CV_R2))
rownames(OLS_Summ) <- c("OLS_CV_Stats")
colnames(OLS_Summ) <- c("MAE","RMSE","R^2")

OLS_Summ %>% kable()

```

1 can see the OLS models cross-validated performance summary statistics above. The OLS model, modelling the percentage change in logTotalValue reached an R-squared of 88.8%, with root mean square error of 52.2% & mean absolute error of 37.7%, giving a solid baseline model.


#### d) Now use Penalized linear Models - LASSO / Ridge / Elastic Net

  -   Does best model improve results vs c) ?
  
Below I specify 3 models using train() from caret, a LASSO, a Ridge, & their combination, an Elastic Net. I do so by using the "glmnet" method, & preProcess the data my normalizing it to standard normal distributions. I specify a tuning grid for each model, searching for the optimal lambda parameter, the penalty term essentially controlling how strongly beta coefficients are shrinked towards 0. Note, readers can find summary statistics for these models under exercise 1.e) together with the "simplest still good enough" models. Eventually, the full Elastic Net model improved the OLS model the most, reaching R-squared of 89.7%, RMSE of 50.3% & MAE of 37.7%, however is very closely followed by LASSO. The LASSO and Elastic Net differ in results by 0.3% MAE, 0.01% in R-squared, & nothing in terms of RMSE. Overall, the Elastic Net represents a 2% RMSE improvement vs the baseline OLS model, improving results. 


```{r d , message = F, warning = F, echo = T}

### CARET versions ----
### LASSO ----

lasso_tune_grid <- expand.grid(
  "alpha" = c(1),
  "lambda" = 10^seq(2,-5,length=100)
)

set.seed(1234)
lasso_fit <- train(
  Formula,
  data = data_train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid = lasso_tune_grid,
  trControl = train_control
)

### RIDGE ----
ridge_tune_grid <- expand.grid(
  "alpha" = c(0),
  "lambda" = 10^seq(2,-5,length=100))

set.seed(1234)
ridge_fit <- train(
  Formula,
  data = data_train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid = ridge_tune_grid,
  trControl = train_control
)

### Elastic Net ----
enet_tune_grid <- expand.grid(
  "alpha" = seq(0, 1, by = 0.1),
  "lambda" = union(lasso_tune_grid[["lambda"]], ridge_tune_grid[["lambda"]]))

set.seed(1234)
  enet_fit <- train(
    Formula,
    data = data_train,
    method = "glmnet",
    preProcess = c("center", "scale"),
    tuneGrid = enet_tune_grid,
    trControl = train_control
  )  

#enet_fit$results %>% arrange(RMSE)

# Based on summary table of Cross-Validated results above, 
  #   OLS has lowest MAE, Elastic Net has Lowest RMSE & Highest R^2, 
      # while dropping 17 variables vs the OLS model


```

#### e)  Which model is "simplest one still good enough" ? ----

Following from the models above, I search for the "simplest still good enough" model specifications for the penalized models above. This means I look for the model that is on average as good as the truly best model minus 1 standard error of the chosen performance metric. I apply the above by re-specifying my train_control parameters, this time including the selectionFunction = "oneSE" argument, which does exactly what I just described, & then re-running all 3 penalized models. Please see the models' summary statistic below, beneath the code snippet.  

```{r e , message = F, warning = F, echo = T}
# e)  Which model is "simplest one still good enough" ? ----
    #   Explore adding: selectionFunction = "oneSE" to trainControl

# OLS beats all simplest still good enough models 
  #   -> Elastic Net Came very close in RMSE & R2 while using 48 less predictors
    #     vs OLS difference in RMSE: 0.0031 / R2: 0.0013
      # i.e. on average 0.3% more off
    #     vs Elastic Net RMSE; 0.0052 / R2: 0.0022

train_control_1se <- trainControl(method = "cv",number = 10,
                                 verboseIter = FALSE,
                                 selectionFunction = "oneSE")  

lasso_tune_grid <- expand.grid(
  "alpha" = c(1),
  "lambda" = 10^seq(2,-5,length=100)
)

set.seed(1234)
  lasso_fit_1se <- train(
    Formula,
    data = data_train,
    method = "glmnet",
    preProcess = c("center", "scale"),
    tuneGrid = lasso_tune_grid,
    trControl = train_control_1se
  )

### RIDGE ----
ridge_tune_grid <- expand.grid(
  "alpha" = c(0),
  "lambda" = 10^seq(2,-5,length=100)  
)

set.seed(1234)
  ridge_fit_1se <- train(
    Formula,
    data = data_train,
    method = "glmnet",
    preProcess = c("center", "scale"),
    tuneGrid = ridge_tune_grid,
    trControl = train_control_1se
  )

### Elastic Net ----
enet_tune_grid <- expand.grid(
  "alpha" = seq(0, 1, by = 0.1),
  "lambda" = union(lasso_tune_grid[["lambda"]], ridge_tune_grid[["lambda"]])
)

set.seed(1234)
  enet_fit_1se <- train(
    Formula,
    data = data_train,
    method = "glmnet",
    preProcess = c("center", "scale"),
    tuneGrid = enet_tune_grid,
    trControl = train_control_1se
  )  

#### Summary ----

resample_profile <- resamples(
  list("OLS" = ols_model,
       "RIDGE" = ridge_fit,
       "LASSO" = lasso_fit,
       "Elastic Net" = enet_fit,
       "RIDGE_1se" = ridge_fit_1se,
       "LASSO_1se" = lasso_fit_1se,
       "Elastic_Net_1se" = enet_fit_1se)) 

Models <- list("OLS" = ols_model,"RIDGE" = ridge_fit,
               "LASSO" = lasso_fit,"Elastic Net" = enet_fit,
               "RIDGE_1se" = ridge_fit_1se,"LASSO_1se" = lasso_fit_1se,
               "Elastic_Net_1se" = enet_fit_1se)

ols_coeffs      <- as.matrix(coef(ols_model$finalModel))
lasso_coeffs    <- as.matrix(coef(lasso_fit$finalModel, lasso_fit$bestTune$lambda))
ridge_coeffs    <- as.matrix(coef(ridge_fit$finalModel, ridge_fit$bestTune$lambda))
enet_coeffs     <- as.matrix(coef(enet_fit$finalModel, enet_fit$bestTune$lambda))
lasso1se_coeffs <- as.matrix(coef(lasso_fit_1se$finalModel, lasso_fit_1se$bestTune$lambda))
ridge1se_coeffs <- as.matrix(coef(ridge_fit_1se$finalModel, ridge_fit_1se$bestTune$lambda))
enet1se_coeffs  <- as.matrix(coef(enet_fit_1se$finalModel, enet_fit_1se$bestTune$lambda))

Nr_Vars <- list(
  "OLS"   = sum(ols_coeffs != 0, na.rm = T),
  "RIDGE" = sum(ridge_coeffs != 0),
  "LASSO" = sum(lasso_coeffs != 0),
  "E_Net" = sum(enet_coeffs != 0),
  "RIDGE_1se" = sum(ridge1se_coeffs != 0),
  "LASSO_1se" = sum(lasso1se_coeffs != 0),
  "E_Net_1se" = sum(enet1se_coeffs != 0))


Penalized_SummTable <- lapply(names(Models), function(x) {
  tdf <-  resample_profile$values %>% dplyr::select(matches(x))
  tl <- list()
  
  tl[['Regr.Model']] <- x
  tl[['CV_MAE']] <- (tdf %>% dplyr::select(matches("MAE")))[[1]] %>% mean() %>% round(4)
  tl[['CV_RMSE']] <- (tdf %>% dplyr::select(matches("RMSE")))[[1]] %>% mean() %>% round(4)
  tl[['CV_R^2']] <- (tdf %>% dplyr::select(matches("Rsquared")))[[1]] %>% mean() %>% round(4)
  
  return(tl)
}) %>% rbindlist() %>% cbind(Nr_Vars)


Penalized_SummTable %>% kable()

```

1 can see from the table above that the best-performing model is still the original Elastic Net. Among simpler but still good enough models however, the LASSO poses the lowest MAE (38.3%), RMSE (50.8%) & highest R-squared (89.5%), meanwhile using 59 less variables vs the best LASSO model, & 93 less variables then originally specified. Do note that 11 variables returned NAs in OLS due to zero-variances. Thus, among simpler but still good enough models, LASSO is the best choice. 


#### 1. f) Try improve Linear Model w PCA 4 Dim Reduction.

Next up, is trying to improve the Linear Model by first performing principal component analysis. To do so however, I first need to convert my factor variables to series of dummies, from which distances can be calculated. I use dummyVars() from caret to define these dummies, & re-create my original dataframe called tdf. Then I re-create the training & holdout sets, and use prcomp() to evaluate principal components. I found the first 35 PCs to explain 99.9% of total variance, & 60 PCs to explain 99.99% of variance. The number of principal components used to arrive at the best OLS model was 117, so this was the number of PCs I used to re-estimate all penalized models as well. To see a comparison of all PCA-based models vs their original "best" counterparts, please visit the table under 1.h). For the OLS model, the decline performance is substantial: over 13% decline in MAE, over 14% decline in RMSE, & 6.8% decline in R-squared. 


```{r f , message = F, warning = F, echo = T}

    #   Center & Scale variables -> use pcr 2 find optimal number of PCs
    #   Does PCA Improve fit over simple linear models
    #     # Many factor variables -> Include 60-90 PCs in search as well

#    preProcess = c("center", "scale", "pca"),

#   1) Pre-Processing -> removing factors
      # Other factors -> dummy variable table -> dummyVars
        #   -> will probably result in smaller variances -> hence 60 - 90 PCs

# Using dummyVars to make factor variables -> make new dataframe basically
dummies <- dummyVars(Formula, data = df)
tdf <- cbind("logTotalValue" = df$logTotalValue,predict(dummies,newdata = df) %>% as.data.frame())

#### Sample vs Holdout sets
set.seed(1234)
train_indices <- as.integer(createDataPartition(tdf$logTotalValue, p = 0.3, list = FALSE))
data_train <- tdf[train_indices, ]
data_holdout2 <- tdf[-train_indices, ]

#### OLS w PCA ####
PCA_tdf <- prcomp(tdf)
Summary <- PCA_tdf %>% summary()
PCA_Summ_df <- Summary$importance %>% rbind() %>% as.data.frame() %>% transpose() %>% cbind(index = 1:300)
colnames(PCA_Summ_df) <- c("StDev","Prop%_Variance","CumSum_Prop%_Variance","index")
# PC35 -> 99.9% of total variance -> PC60 = 99.99%

trctrlWithPCA <- trainControl(method = "cv", number = 10, verboseIter = FALSE)
tune_grid <- data.frame(ncomp = 35:120)

set.seed(1234)
  ols_model_pca <- train(
    logTotalValue ~ .,
    data = data_train,
    method = "pcr",
    preProcess = c("center", "scale"),
    tuneGrid = tune_grid,
    trControl = trctrlWithPCA)

#     ncomp = 117 is optimal

```


####  1.g) Applying PCA to penalized models via perProcess -> achieves better fit?

After estimating the optimal number of PCs on the OLS model, this number was used on penalized models, by adding preProcOptions to train_control. Also once removing near-zero variances, 121 Principal Components remained. Overall, penalized models also declined in performance vs their best-fit formulations, though less starkly vs the OLS models. The RIDGE pca model also performed very similarly to the LASSO, while in their original formulations the Ridge model was over 2% worse in MAE & RMSE, and 1% lower in R-squared. Also, the Elastic Net using PCA return an optimal alpha parameter of 1, i.e. it reduced to a LASSO model, returning identical performance metrics as the LASSO pca. As such, the LASSO pca is the best model using Principal Component Analysis.  

```{r g , message = F, warning = F, echo = T}

    #   Include "nzv" to preProcess also -> Drops zero variance features
      # What's your intuition, why is this happening ? 

### LASSO PCA ----

lasso_tune_grid <- expand.grid(
  "alpha" = c(1),
  "lambda" = 10^seq(0,-5.5,length=100))

set.seed(1234)
  lasso_fit_pca <- train(
    logTotalValue ~ .,
    data = data_train,
    method = "glmnet",
    preProcess = c("center", "scale","nzv","pca"),
    tuneGrid = lasso_tune_grid,
    trControl = trainControl(method = "cv",number = 10,preProcOptions = list(pcaComp = 117))
  )

### RIDGE PCA ----
ridge_tune_grid <- expand.grid(
  "alpha" = c(0),
  "lambda" = 10^seq(4,-5.5,length=200)  
)

set.seed(1234)
  ridge_fit_pca <- train(
    logTotalValue ~ .,
    data = data_train,
    method = "glmnet",
    preProcess = c("center", "scale","nzv","pca"),
    tuneGrid = ridge_tune_grid,
    trControl = trainControl(method = "cv",number = 10,preProcOptions = list(pcaComp = 117))
  )

### Elastic Net PCA ----
enet_tune_grid <- expand.grid(
  "alpha" = seq(0.7, 1, by = 0.03),
  "lambda" = union(lasso_tune_grid[["lambda"]], ridge_tune_grid[["lambda"]]))

set.seed(1234)
  enet_fit_pca <- train(
    logTotalValue ~ .,
    data = data_train,
    method = "glmnet",
    preProcess = c("center", "scale","nzv","pca"),
    tuneGrid = enet_tune_grid,
    trControl = trainControl(method = "cv",number = 10,preProcOptions = list(pcaComp = 117))
  )  

```

# h) Select best model trained -> Evaluate your preferred model on test set

```{r h , message = F, warning = F, echo = T}

#### Summary ----

Resamples_w_pca <- resamples(
  list("OLS" = ols_model,
       "OLS_PCA" = ols_model_pca,
       "RIDGE" = ridge_fit,
       "RIDGE_PCA" = ridge_fit_pca,
       "LASSO" = lasso_fit,
       "LASSO_PCA" = lasso_fit_pca,
       "Elastic Net" = enet_fit,
       "Elastic_Net_PCA" = enet_fit_pca
       )) 

Models <- list("OLS" = ols_model       ,"OLS_PCA" = ols_model_pca,
               "RIDGE" = ridge_fit     ,"RIDGE_PCA" = ridge_fit_pca,
               "LASSO" = lasso_fit     ,"LASSO_PCA" = lasso_fit_pca,
               "Elastic Net" = enet_fit,"Elastic_Net_PCA" = enet_fit_pca)


PCA_SummTable <- lapply(names(Models), function(x) {
  tdf <-  Resamples_w_pca$values %>% dplyr::select(matches(x))
  tl <- list()
  
  tl[['Regr.Model']] <- x
  tl[['CV_MAE']] <- (tdf %>% dplyr::select(matches("MAE")))[[1]] %>% mean() %>% round(4)
  tl[['CV_RMSE']] <- (tdf %>% dplyr::select(matches("RMSE")))[[1]] %>% mean() %>% round(4)
  tl[['CV_R^2']] <- (tdf %>% dplyr::select(matches("Rsquared")))[[1]] %>% mean() %>% round(4)
  
  return(tl)
}) %>% rbindlist()


PCA_SummTable %>% kable()

```

Due to the reasons outlined above, the best-fit Elastic Net model is my final recommendation, boasting the lowest MAE (0.3768) , RMSE (0.5025) & highest R-squared metrics (89.68%). Only out of sheer curiosity, I also test the Elastic Net model using PCA on the holdout set. Below 1 can see the Mean Absolute Error & Root Mean Square Error statistics on the holdout. The best-fit Elastic Net model was stable in performance, with MAE 0.3777 & RMSE 0.5031, meaning we managed to not overfit my model to the data. The Elastic Net model using PCA tested RMSE = 0.5386, &  MAE = 0.399, even slightly improving vs its cross-validated results visible above.


```{r h2 , message = F, warning = F, echo = T}
data_holdout$pred_E.Net <- predict(enet_fit,newdata = data_holdout)
Hd_RMSE <- RMSE(data_holdout$pred_E.Net,data_holdout$logTotalValue) %>% round(4)
Hd_MAE <-  MAE(data_holdout$pred_E.Net,data_holdout$logTotalValue) %>% round(4)

data_holdout2$pred_E.NetPca <- predict(enet_fit_pca,newdata = data_holdout2)
Hd_RMSE_Pc <- RMSE(data_holdout2$pred_E.NetPca,data_holdout2$logTotalValue) %>% round(4)
Hd_MAE_Pc <-  MAE(data_holdout2$pred_E.NetPca,data_holdout2$logTotalValue) %>% round(4)

HoldoutSumms <- rbind(cbind(Hd_RMSE,Hd_MAE),cbind(Hd_RMSE_Pc,Hd_MAE_Pc)) %>% as.data.frame()

rownames(HoldoutSumms) <- c("Best_Elastic_Net","Elastic_Net_PCA")
HoldoutSumms %>% kable()
```

#### 2) Clustering on the USArrests dataset 

The 2nd chapter of this exercise uses the USArrests dataset, to cluster US states. As part of understanding the data, I use the GGally librarys' ggpairs function to get scatter plot, variable density-plots & correlations simultaneously. The plots below 1st weakly skewed variables except for Urban Population, & moderate to strong correlations among Murders & Assault, Murders & Rape, and Assault & Rape. 1 could argue the reason for this is some assaults escalating to rapes to occur, & some rapes to also result in murders.

```{r e2, message = F, warning = F, echo = T, fig.align='center'}
  #   Task: Apply Clustering -> make sense of clusters with PCA
  #   Data used in Class

library(tidyverse)
library(caret)
library(skimr)
library(janitor)
library(factoextra) # provides nice functions for visualizing the output of PCA
library(NbClust) # for choosing the optimal number of clusters
library(knitr)
library(kableExtra)

theme_set(theme_tufte())

data <- USArrests
ggally <- GGally::ggpairs(data, title = "USArrests data Scatters, Densities & Correlations")

ggally

```

#### 2. a) Any data pre-processing you may / should / could / would do ? 

To be able to calculate Principal Components, or perform K-means clustering, I scale the data with the stats librarys' scale() function.

```{r 2a , message = F, warning = F, echo = T}
      #   Scale variables 
data_scaled <- scale(data) %>% as.data.frame()
```

#### 2. b) Determine optimal number of clusters as indicated by NbClust heuristics 

To determine optimal number of clusters for K-means clustering, I use the NbClust() function, trying any number of clusters from 2 to 10. 

```{r 2b , message = F, warning = F, echo = T,results="hide", fig.align='center', fig.height=3}

nb <- NbClust(data_scaled, method = "kmeans", min.nc = 2, max.nc = 10, index = "all")

Cluster <- nb$Best.nc["Number_clusters",] %>% cbind() %>% as.data.frame()
colnames(Cluster) <- "Optimal_Cluster_Nr"

NrClustsRec <- Cluster %>% 
    ggplot(aes(x = Optimal_Cluster_Nr)) + 
    geom_bar(color = "red", fill = "navy") +
    scale_x_continuous(labels = c(0:10), breaks = c(0:10)) +
    scale_y_continuous(labels = c(1:11), breaks = c(1:11)) + 
    labs(title = "Frequency of Recommended Nr. Of Clusters",
         y = "Nr. Times Reccommended",
         x = "Nr. of Clusters Recommended")

NrClustsRec 

```

The plot above depicts the distribution of voted cluster numbers. 2 is overwhelmingly in favor & is therefore my choice.

#### 2 c) Use K-means to Cluster states using Nr. Clusters found in b)

Below I use the most voted number of clusters (obtained fomr above) to cluster observations, with 30 starting points. I add the resulting cluster designations to my original data frame, & color the clusters while depicting Urban Population vs Murder, Assault & Rape. The plots below show that largely, K-means seems to assign clusters based on these 3 variables. 

```{r 2c , message = F, warning = F, echo = T, fig.height=3}

km <- kmeans(data_scaled, centers = nb$Best.partition %>% unique() %>% length(),
             nstart = 30)

data_w_clusters <- mutate(data, cluster = factor(km$cluster))

# Murder
Murder <- data_w_clusters %>% 
  ggplot(aes(x = UrbanPop, y = Murder,  color = cluster)) +
  geom_point( )

# Assault
Assault <- ggplot(data_w_clusters, aes(x = UrbanPop, y = Assault, color = cluster)) +geom_point()

# Rape
Rape <- ggplot(data_w_clusters, aes(x = UrbanPop, y = Rape, color = cluster)) +  geom_point()

grid.arrange(Murder, Assault, Rape, ncol = 3)

```

#### 2 d) Do PCA & Get 1st 2 PC coordinates 4 all observations: 

In this final section, the 1st 2 Principal Components are obtain from the raw data, & data points are visualized from perspective of the 2 principal components, after assigned clusters are added to the dataframe. The plot below shows a clear line of de-markation at ca. PC1 = -0.1. The question remaining is what these principal components do most significantly take into account, as well as what seperates these identified clusters.

```{r 2d , message = F, warning = F, echo = T }
pca_result <- prcomp(data,scale = T)

Cities <- rownames(pca_result$x)

first_two_pc <- pca_result$x[,1:2] %>% as.data.frame()
first_two_pc <- first_two_pc %>% 
  mutate(cluster = factor(km$cluster),City = Cities)
rownames(first_two_pc) <- Cities

    #   Plot clusters of choice along co-ordinate system of 1st 2 PCs

PC12Plot <- first_two_pc %>% ggplot(aes(x = PC1,y = PC2, color = cluster)) +
  geom_point() +geom_text(label = Cities, size = 3, hjust = -0.1)

  # Very clear Seperation along PC1 

#   How do clusters relate to these?
ClusterTable <- data_w_clusters %>% group_by(cluster) %>%
  summarize(StateCount = n(),
            AvgMurder = mean(Murder)     ,AvgAssault = mean(Assault),
            AvgUrbanPop = mean(UrbanPop) ,AvgRape = mean(Rape))
  # Cluster 2 ca. 3x Murder / 2x Assault / 2x Rape
    # Safe vs Unsafe seems an intuitive cluster title 
      # Weights used to project observations along PCs helps this further:

PC12_Weights <- pca_result$rotation[,1:2] %>% round(4)

PC1_Cont <- fviz_contrib(pca_result, "var", axes = 1) # Crime-related
PC2_Cont <- fviz_contrib(pca_result, "var", axes = 2) # Population-related

PC12Plot


ClusterTable %>% kable()

PC12_Weights %>% kable()

grid.arrange(PC1_Cont,PC2_Cont, ncol = 2)


```

A 1st table above calculates variable mean by cluster. 1 can see that cluster 1 vs cluster 2, the average number of murders is 2.5x, average number of assaults & rapes are 2x. Thus, 1 way to interpret the clusters may be Safe vs Unsafe. The 2nd table shows the explicit weights used per variable to derive observation coordinates from perspective of the first 2 Principal Components, while the bar charts that follow depict variable significance/contribution for a PC. Indeed, 1 can see that the 1st Principal Component primarily takes into account crime-related variables, while the 2nd PC overwhelmingly focuses on Urban Population.


# Appendices

### Raw Histograms, Box- & Scatterplots

```{r App 1 , message = F, warning = F, echo = T, results = "hide", fig.height=2.5, fig.width= 5}

Hists() 

```


### Transformed Box- & Scatterplots

```{r App 2 , message = F, warning = F, echo = T, results = "hide", fig.height=2.5, fig.width= 5}

Boxes_n_Scatters()

```
