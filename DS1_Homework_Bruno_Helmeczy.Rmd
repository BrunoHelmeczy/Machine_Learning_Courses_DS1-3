---
title: "Regressions, PCA, Clustering"
author: "Bruno Helmeczy"
date: "21/02/2021"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
  pdf_document: default
---

```{r setup, message = F, warning = F, echo = T}
library(tidyverse)
library(datasets)
library(MASS)
library(ISLR)
library(caret)
library(data.table)
library(ggthemes)
library(ggridges)
#library()
library(knitr)
library(kableExtra)


rm(list=ls())
options(scipen=999)
theme_set(theme_tufte())

df <- readRDS(url('http://www.jaredlander.com/data/manhattan_Train.rds')) %>%
  mutate(logTotalValue = log(TotalValue)) %>%
  drop_na() %>% dplyr::select(-TotalValue)

```


### 1) Supervised Learning with Penalized Models & PCA

```{r helpers, message = F, warning = F, echo = T}
### 1) Supervised Learning with Penalized Models & PCA ----
    # Property Values in Manhattan - R 4 Everyone
    #   Predict log-Property Value = logTotalValue

# Function to check distinct values & class of all columns - anytime
ColsUniques <- function(dataframe = df) {
  rbindlist(lapply(1:length(dataframe), function(x) {
    tl <- list()
    tl[['name']] <- colnames(dataframe)[x]
    tl[['distinct']] <- nrow(unique(dataframe[,x]))
    tl[['class']] <- dataframe[,x][[1]] %>% class()
    return(tl)
  }))  
}
ColsUni1 <- ColsUniques()

    # Zone Distr 2-4 -> 90+ %  Missing value
df$ZoneDist4 <- NULL
df$ZoneDist3 <- NULL
df$ZoneDist2 <- NULL

# Binary Factors -> Yes = 1 No = 0
df <- df %>% mutate(
  IrregularLot     = ifelse(IrregularLot == "Yes",1,0),
  Landmark         = ifelse(Landmark == "Yes",1,0),
  HistoricDistrict = ifelse(HistoricDistrict == "Yes" ,1,0),
  High             = ifelse(High == T,1,0)
) %>% mutate(
  Council    = as.factor(Council),
  PolicePrct = as.factor(PolicePrct),
  HealthArea = as.factor(HealthArea)
)

ColsUni2 <- ColsUniques()
```


#### 1.a) Short EDA on data -> Find possible predictors 

```{r a , message = F, warning = F, echo = T, fig.align= 'center', fig.height=3,fig.width=6}

# 2 functions to check all histograms / boxplots / LOESS-es in dataset
Hists <- function() {
  ColsUni2 <- ColsUniques()
  
  lapply(df %>% dplyr::select(-matches("id")) %>% colnames(),function(x) {
    if( ( ColsUni2$class[ColsUni2$name == x] %in% c("integer","numeric") ) & (df[,x] %>% unique() %>% nrow() >2 )  ) {
    plot <- df %>% ggplot(aes_string(x = x)) + 
        geom_histogram( color = "red", fill = "blue") + theme_tufte() + 
        labs(title = paste0("NYC Properties ",x," Distribution")) 
    print(plot)
    } else {
      plot <- df %>% ggplot(aes_string(x = x ) ) + 
        geom_bar(color = "red", fill = "blue") + coord_flip() + theme_tufte() + 
        labs(title = paste0("NYC Properties ",x," Distribution"))  
      print(plot)
    }
  })
  
}

Boxes_n_Scatters <- function() {
  ColsUni1 <- ColsUniques()
  
# Box Plots
#  x <- "SchoolDistrict"
  lapply(df %>% dplyr::select(-matches("id|logTotalValue")) %>%  colnames(), function(x) {
      if (  (ColsUni1$class[ColsUni1$name == x] %in% c("factor","logical") ) | (df[,x] %>% unique() %>% nrow() == 2)  ) {
    plot <- df %>% ggplot()  +
          geom_boxplot(aes_string(y = x, x = "logTotalValue", group = x),color = "red", fill = "blue") +
          theme_tufte() +  
          labs(title = paste0("Title ",x))
    print(plot)
    print(paste0("Printing plot: ",x))
      } 
    })
  
# Scatters 
  xvars <- df %>% dplyr::select(
    matches(paste(ColsUni1$name[ColsUni1$class %in% 
                                  c("numeric","integer")],collapse = "|"))) %>%
    dplyr::select(-matches("id|logTotalValue")) %>% colnames()
  
  for (i in xvars) {
    plot <- df[,c(i,"logTotalValue")] %>% 
      ggplot(aes_string(x = i, y = "logTotalValue")) +
      geom_smooth() + geom_point(size = 0.1)  +
      labs(title = paste0("Title ",i))
    
    print(plot)  
    print(paste0("Printing plot: ",i))
  }
  
}

# By default with raw variables, we see Area-related variables to be highly skewed, ----
  #   while in all cases there are a number of zero values. When such areas values are zero,
  #   it implies there is no such aspect of the building, e.g. no garage.
  #   It is in and of itself a value driver if a garage exists 
      #   (reflected by all '_Zero' ending variables) 
  #   While also flags those observations where the raw values were zero before log-transforms
    # we thus can interpret coefficient parameters as 
      #   % higher area being associated with on average % higher total value  

df <- df %>% mutate(
  BuiltFAR_Zero   = ifelse(BuiltFAR == 0, 1, 0),
  BldgDepth_Zero  = ifelse(BldgDepth == 0,1,0),
  BldgFront_Zero  = ifelse(BldgFront == 0 , 1,0),
  LotDepth_Zero   = ifelse(LotDepth == 0,1,0),
  LotFront_Zero   = ifelse(LotFront == 0,1,0),
  UnitsTotal_Zero = ifelse(UnitsTotal == 0,1,0),
  UnitsRes_Zero   = ifelse(UnitsRes == 0,1,0),
  NumFloors_Zero  = ifelse(NumFloors == 0,1,0),
  NumBldgs_Zero   = ifelse(NumBldgs == 0,1,0),
  OtherArea_Zero  = ifelse(OtherArea == 0,1,0),
  FactryArea_Zero = ifelse(FactryArea == 0,1,0),
  StrgeArea_Zero  = ifelse(StrgeArea == 0,1,0),
  GarageArea_Zero = ifelse(GarageArea == 0,1,0),
  RetailArea_Zero = ifelse(RetailArea == 0,1,0),
  OfficeArea_Zero = ifelse(OfficeArea == 0,1,0),
  ResArea_Zero    = ifelse(ResArea == 0,1,0),
  ComArea_Zero    = ifelse(ComArea == 0,1,0),
  BldgArea_Zero   = ifelse(BldgArea == 0,1,0),
  LotArea_Zero    = ifelse(LotArea == 0,1,0),
  Easements       = ifelse(Easements > 0,1,0)
)

# log-transforms: ----
# BuiltFAR, BldgDepth, BldgFront, LotDepth, LotFront, UnitTotal, UnitsRes,
# NumFloors, NumBldgs, OtherArea, FactryArea, StrgeArea, GarageArea, RetailArea,
# OfficeArea, ResArea, ComArea, BldgArea, LotArea, Easements


df <- df %>% mutate(
  BuiltFAR_ln   = log(BuiltFAR    + 0.01),
  BldgDepth_ln  = log(BldgDepth   + 0.01),
  BldgFront_ln  = log(BldgFront   + 0.01),
  LotDepth_ln   = log(LotDepth    + 0.01),
  LotFront_ln   = log(LotFront    + 0.01),
  UnitsTotal_ln = log(UnitsTotal  + 0.01),
  UnitsRes_ln   = log(UnitsRes    + 0.01),
  NumFloors_ln  = log(NumFloors   + 0.01),
  NumBldgs_ln   = log(NumBldgs    + 0.001),
  OtherArea_ln  = log(OtherArea   + 0.01),
  FactryArea_ln = log(FactryArea  + 0.01),
  StrgeArea_ln  = log(StrgeArea   + 0.01),
  GarageArea_ln = log(GarageArea  + 0.01),
  RetailArea_ln = log(RetailArea  + 0.01),
  OfficeArea_ln = log(OfficeArea  + 0.01),
  ResArea_ln    = log(ResArea     + 0.01),
  ComArea_ln    = log(ComArea     + 0.01),
  BldgArea_ln   = log(BldgArea    + 0.01),
  LotArea_ln    = log(LotArea     + 0.01)) %>% 
  dplyr::select(-c(BuiltFAR, BldgDepth, BldgFront, LotDepth, LotFront, UnitsTotal,
            UnitsRes, NumFloors, NumBldgs, OtherArea, FactryArea, StrgeArea,
            GarageArea, RetailArea, OfficeArea, ResArea, ComArea, BldgArea, LotArea))

# Correls w logTotalValue
Cols <- ColsUniques()

Y_Cors <- df %>% dplyr::select(matches(Cols$name[Cols$class %in% c("numeric", "integer")])) %>% 
  dplyr::select(-matches("id|logTotalValue")) %>% colnames() %>% 
  lapply(function(x) {
    tl <- list()
    tl[['Colname']] <- x
    tl[['Corr_w_Y_abs']] <- cor(df[,x],df[,"logTotalValue"]) %>% round(2) %>% abs()
    return(tl)
  }) %>% rbindlist() %>% as.data.frame() %>% arrange(desc(Corr_w_Y_abs))


TopCors <- Y_Cors[1:15,] %>% ggplot(aes(x = reorder(Colname, Corr_w_Y_abs), y = Corr_w_Y_abs,
                      color = "red")) +
  geom_point(size = 8) + 
  geom_col(width = 0.1, position = "dodge", fill  = "red") +
  geom_text(aes(label = Corr_w_Y_abs), size = 3, color = "black") + 
  theme_tufte() + 
  theme(legend.position = c(2,2),legend.title = element_text(size = 8)
        ,legend.text = element_text(size = 8),legend.key.size = unit(2,"mm")) +
  coord_flip() +
  labs(title = "Top Correlations with Target-Y Variable",
       y = "Absolute Correlation",
       x = "Continuous X variables")
  
# Top 3 correlating variables are LotArea, LotFront & Commercial Area, 
  # w Number of Units in a Building & total Building area making the top 5

TopCors
```






#### 1.b) Create Training - Test sets -> 70% test - 30% training 

```{r b , message = F, warning = F, echo = T}

#### Sample vs Holdout sets
set.seed(1234)
train_indices <- as.integer(createDataPartition(df$logTotalValue, p = 0.3, list = FALSE))
data_train <- df[train_indices, ]
data_holdout <- df[-train_indices, ]

# train control is 10 fold cross validation
train_control <- trainControl(method = "cv",number = 10,verboseIter = FALSE)  


```


#### c) Lin. Regr. 2 predict logTotalValue -> 10-fold CV to assess predictive power

```{r c , message = F, warning = F, echo = T}

# We can also expect that a garages value increases, 
    # given how large the building is, how much commercial area is there, etc.
  #   all-in-all I hypothesize a positive feedback loop between all area related variables
    #   I model this via interactions between all area related variables 
        #   -> ( 10 * 9 ) / 2 = 45 interactions 

Vars <- df %>% dplyr::select(matches("area_ln")) %>% colnames()

Interactions <- NULL
Counter <- 1
for (i in 1:length(Vars)) {
  for (j in (i+1):(length(Vars)) ) {
    Interactions[Counter] <- paste(Vars[i],Vars[j],sep =  " * ")
    Counter <- Counter + 1
  }
}
Interactions <- paste(Interactions[1:45],collapse =  " + " ) 

Vars <- df %>% dplyr::select(-matches("id|logTotalValue")) %>% colnames()
Formula <- formula(paste0("logTotalValue ~",paste(Vars, collapse = " + ")," + ", Interactions))

#### OLS ####
set.seed(1234)
ols_model <- train(
  formula(Formula),
  data = data_train,
  method = "lm",
  trControl = train_control)

CV_RMSE <- ols_model$resample[,1] %>% mean()
CV_MAE <- ols_model$resample[,3] %>% mean()
CV_R2 <- ols_model$resample[,2] %>% mean()

data_holdout$ols_pred <- predict(ols_model, newdata = data_holdout)
# Holdout
Hd_MAE <- MAE(data_holdout$ols_pred, data_holdout$logTotalValue)
Hd_RMSE <- RMSE(data_holdout$ols_pred, data_holdout$logTotalValue)

# OLS Summary - Cross-Validation & Holdout data
OLS_Summ <- as.data.frame(rbind(cbind(CV_MAE,CV_RMSE,CV_R2),cbind(Hd_MAE,Hd_RMSE,CV_R2)))
rownames(OLS_Summ) <- c("CV_Stats","Holdout_Stats")
colnames(OLS_Summ) <- c("MAE","RMSE","R^2")

OLS_Summ %>% kable()

```


#### d) Now use Penalized linear Models - LASSO / Ridge / Elastic Net

  -   Does best model improve results vs c)


```{r d , message = F, warning = F, echo = T}

### CARET versions ----
### LASSO ----

lasso_tune_grid <- expand.grid(
  "alpha" = c(1),
  "lambda" = 10^seq(2,-5,length=100)
)

set.seed(1234)
lasso_fit <- train(
  Formula,
  data = data_train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid = lasso_tune_grid,
  trControl = train_control
)

lasso_fit %>% ggplot() +xlim(c(0,1))

lasso_fit$results %>% ggplot(aes(x = lambda)) +
  geom_line(aes(y = RMSE), color = "blue") +
  geom_line(aes(y = Rsquared), color = "red") + 
  geom_line(aes(y = MAE), color = "green") + xlim(c(0,1))

### RIDGE ----
ridge_tune_grid <- expand.grid(
  "alpha" = c(0),
  "lambda" = 10^seq(2,-5,length=100))

set.seed(1234)
ridge_fit <- train(
  Formula,
  data = data_train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid = ridge_tune_grid,
  trControl = train_control
)

### Elastic Net ----
enet_tune_grid <- expand.grid(
  "alpha" = seq(0, 1, by = 0.1),
  "lambda" = union(lasso_tune_grid[["lambda"]], ridge_tune_grid[["lambda"]]))

set.seed(1234)
  enet_fit <- train(
    Formula,
    data = data_train,
    method = "glmnet",
    preProcess = c("center", "scale"),
    tuneGrid = enet_tune_grid,
    trControl = train_control
  )  

#enet_fit$results %>% arrange(RMSE)

# Based on summary table of Cross-Validated results above, 
  #   OLS has lowest MAE, Elastic Net has Lowest RMSE & Highest R^2, 
      # while dropping 17 variables vs the OLS model


```

#### e)  Which model is "simplest one still good enough" ? ----


```{r e , message = F, warning = F, echo = T}
# e)  Which model is "simplest one still good enough" ? ----
    #   Explore adding: selectionFunction = "oneSE" to trainControl

# OLS beats all simplest still good enough models 
  #   -> Elastic Net Came very close in RMSE & R2 while using 48 less predictors
    #     vs OLS difference in RMSE: 0.0031 / R2: 0.0013
      # i.e. on average 0.3% more off
    #     vs Elastic Net RMSE; 0.0052 / R2: 0.0022

train_control_1se <- trainControl(method = "cv",number = 10,
                                 verboseIter = FALSE,
                                 selectionFunction = "oneSE")  

lasso_tune_grid <- expand.grid(
  "alpha" = c(1),
  "lambda" = 10^seq(2,-5,length=100)
)

set.seed(1234)
  lasso_fit_1se <- train(
    Formula,
    data = data_train,
    method = "glmnet",
    preProcess = c("center", "scale"),
    tuneGrid = lasso_tune_grid,
    trControl = train_control_1se
  )

lasso_fit %>% ggplot() +xlim(c(0,1))

lasso_fit$results %>% ggplot(aes(x = lambda)) +
  geom_line(aes(y = RMSE), color = "blue") +
  geom_line(aes(y = Rsquared), color = "red") + 
  geom_line(aes(y = MAE), color = "green") + xlim(c(0,1))

### RIDGE ----
ridge_tune_grid <- expand.grid(
  "alpha" = c(0),
  "lambda" = 10^seq(2,-5,length=100)  
)

set.seed(1234)
  ridge_fit_1se <- train(
    Formula,
    data = data_train,
    method = "glmnet",
    preProcess = c("center", "scale"),
    tuneGrid = ridge_tune_grid,
    trControl = train_control_1se
  )

### Elastic Net ----
enet_tune_grid <- expand.grid(
  "alpha" = seq(0, 1, by = 0.1),
  "lambda" = union(lasso_tune_grid[["lambda"]], ridge_tune_grid[["lambda"]])
)

set.seed(1234)
  enet_fit_1se <- train(
    Formula,
    data = data_train,
    method = "glmnet",
    preProcess = c("center", "scale"),
    tuneGrid = enet_tune_grid,
    trControl = train_control_1se
  )  

#### Summary ----

resample_profile <- resamples(
  list("OLS" = ols_model,
       "RIDGE" = ridge_fit,
       "LASSO" = lasso_fit,
       "Elastic Net" = enet_fit,
       "RIDGE_1se" = ridge_fit_1se,
       "LASSO_1se" = lasso_fit_1se,
       "Elastic_Net_1se" = enet_fit_1se)) 

Models <- list("OLS" = ols_model,"RIDGE" = ridge_fit,
               "LASSO" = lasso_fit,"Elastic Net" = enet_fit,
               "RIDGE_1se" = ridge_fit_1se,"LASSO_1se" = lasso_fit_1se,
               "Elastic_Net_1se" = enet_fit_1se)

ols_coeffs      <- as.matrix(coef(ols_model$finalModel))
lasso_coeffs    <- as.matrix(coef(lasso_fit$finalModel, lasso_fit$bestTune$lambda))
ridge_coeffs    <- as.matrix(coef(ridge_fit$finalModel, ridge_fit$bestTune$lambda))
enet_coeffs     <- as.matrix(coef(enet_fit$finalModel, enet_fit$bestTune$lambda))
lasso1se_coeffs <- as.matrix(coef(lasso_fit_1se$finalModel, lasso_fit_1se$bestTune$lambda))
ridge1se_coeffs <- as.matrix(coef(ridge_fit_1se$finalModel, ridge_fit_1se$bestTune$lambda))
enet1se_coeffs  <- as.matrix(coef(enet_fit_1se$finalModel, enet_fit_1se$bestTune$lambda))

Nr_Vars <- list(
  "OLS"   = sum(ols_coeffs != 0, na.rm = T),
  "RIDGE" = sum(ridge_coeffs != 0),
  "LASSO" = sum(lasso_coeffs != 0),
  "E_Net" = sum(enet_coeffs != 0),
  "RIDGE_1se" = sum(ridge1se_coeffs != 0),
  "LASSO_1se" = sum(lasso1se_coeffs != 0),
  "E_Net_1se" = sum(enet1se_coeffs != 0))


Penalized_SummTable <- lapply(names(Models), function(x) {
  tdf <-  resample_profile$values %>% dplyr::select(matches(x))
  tl <- list()
  
  tl[['Regr.Model']] <- x
  tl[['CV_MAE']] <- (tdf %>% dplyr::select(matches("MAE")))[[1]] %>% mean() %>% round(4)
  tl[['CV_RMSE']] <- (tdf %>% dplyr::select(matches("RMSE")))[[1]] %>% mean() %>% round(4)
  tl[['CV_R^2']] <- (tdf %>% dplyr::select(matches("Rsquared")))[[1]] %>% mean() %>% round(4)
  
  return(tl)
}) %>% rbindlist() %>% cbind(Nr_Vars)


Penalized_SummTable %>% kable()

```


#### 1. f) Try improve Linear Model w PCA 4 Dim Reduction.


```{r f , message = F, warning = F, echo = T}

    #   Center & Scale variables -> use pcr 2 find optimal number of PCs
    #   Does PCA Improve fit over simple linear models
    #     # Many factor variables -> Include 60-90 PCs in search as well

#    preProcess = c("center", "scale", "pca"),

#   1) Pre-Processing -> removing factors
      # Other factors -> dummy variable table -> dummyVars
        #   -> will probably result in smaller variances -> hence 60 - 90 PCs

# Using dummyVars to make factor variables -> make new dataframe basically
dummies <- dummyVars(Formula, data = df)
tdf <- cbind("logTotalValue" = df$logTotalValue,predict(dummies,newdata = df) %>% as.data.frame())

#### Sample vs Holdout sets
set.seed(1234)
train_indices <- as.integer(createDataPartition(tdf$logTotalValue, p = 0.3, list = FALSE))
data_train <- tdf[train_indices, ]
data_holdout2 <- tdf[-train_indices, ]

#### OLS w PCA ####
PCA_tdf <- prcomp(tdf)
Summary <- PCA_tdf %>% summary()
PCA_Summ_df <- Summary$importance %>% rbind() %>% as.data.frame() %>% transpose() %>% cbind(index = 1:300)
colnames(PCA_Summ_df) <- c("StDev","Prop%_Variance","CumSum_Prop%_Variance","index")

PCA_Summ_df %>% ggplot(aes(x = index)) +
  geom_line(aes(y = `Prop%_Variance`)) + xlim(c(0,30)) +
  geom_point(aes(y = `Prop%_Variance`))

PCA_Summ_df %>% ggplot(aes(x = index)) +
  geom_point(aes(y = `CumSum_Prop%_Variance`)) +
  geom_line(aes(y = `CumSum_Prop%_Variance`)) + xlim(c(0,30))

# PC35 -> 99.9% of total variance -> PC60 = 99.99%


trctrlWithPCA <- trainControl(method = "cv", number = 10, verboseIter = TRUE)
tune_grid <- data.frame(ncomp = 35:120)

set.seed(1234)
  ols_model_pca <- train(
    logTotalValue ~ .,
    data = data_train,
    method = "pcr",
    preProcess = c("center", "scale"),
    tuneGrid = tune_grid,
    trControl = trctrlWithPCA)

#     ncomp = 117 is optimal


```


####  1.g) Applying PCA to penalized models via perProcess -> achieves better fit?

```{r g , message = F, warning = F, echo = T}

    #   Include "nzv" to preProcess also -> Drops zero variance features
      # What's your intuition, why is this happening ? 

### LASSO PCA ----

lasso_tune_grid <- expand.grid(
  "alpha" = c(1),
  "lambda" = 10^seq(0,-5.5,length=100))

set.seed(1234)
  lasso_fit_pca <- train(
    logTotalValue ~ .,
    data = data_train,
    method = "glmnet",
    preProcess = c("center", "scale","nzv","pca"),
    tuneGrid = lasso_tune_grid,
    trControl = trainControl(method = "cv",number = 10,preProcOptions = list(pcaComp = 117))
  )

### RIDGE PCA ----
ridge_tune_grid <- expand.grid(
  "alpha" = c(0),
  "lambda" = 10^seq(4,-5.5,length=200)  
)

set.seed(1234)
  ridge_fit_pca <- train(
    logTotalValue ~ .,
    data = data_train,
    method = "glmnet",
    preProcess = c("center", "scale","nzv","pca"),
    tuneGrid = ridge_tune_grid,
    trControl = trainControl(method = "cv",number = 10,preProcOptions = list(pcaComp = 117))
  )

ridge_fit$results %>% ggplot(aes(x = lambda)) +
  geom_line(aes(y = RMSE)) +
  geom_line(aes(y = Rsquared)) + 
  geom_line(aes(y = MAE))

### Elastic Net PCA ----
enet_tune_grid <- expand.grid(
  "alpha" = seq(0.7, 1, by = 0.03),
  "lambda" = union(lasso_tune_grid[["lambda"]], ridge_tune_grid[["lambda"]]))

set.seed(1234)
  enet_fit_pca <- train(
    logTotalValue ~ .,
    data = data_train,
    method = "glmnet",
    preProcess = c("center", "scale","nzv","pca"),
    tuneGrid = enet_tune_grid,
    trControl = trainControl(method = "cv",number = 10,preProcOptions = list(pcaComp = 117))
  )  

```

# h) Select best model trained -> Evaluate your preferred model on test set

```{r h , message = F, warning = F, echo = T}

#### Summary ----

Resamples_w_pca <- resamples(
  list("OLS" = ols_model,
       "OLS_PCA" = ols_model_pca,
       "RIDGE" = ridge_fit,
       "RIDGE_PCA" = ridge_fit_pca,
       "LASSO" = lasso_fit,
       "LASSO_PCA" = lasso_fit_pca,
       "Elastic Net" = enet_fit,
       "Elastic_Net_PCA" = enet_fit_pca
       )) 

Models <- list("OLS" = ols_model       ,"OLS_PCA" = ols_model_pca,
               "RIDGE" = ridge_fit     ,"RIDGE_PCA" = ridge_fit_pca,
               "LASSO" = lasso_fit     ,"LASSO_PCA" = lasso_fit_pca,
               "Elastic Net" = enet_fit,"Elastic_Net_PCA" = enet_fit_pca)


PCA_SummTable <- lapply(names(Models), function(x) {
  tdf <-  Resamples_w_pca$values %>% dplyr::select(matches(x))
  tl <- list()
  
  tl[['Regr.Model']] <- x
  tl[['CV_MAE']] <- (tdf %>% dplyr::select(matches("MAE")))[[1]] %>% mean() %>% round(4)
  tl[['CV_RMSE']] <- (tdf %>% dplyr::select(matches("RMSE")))[[1]] %>% mean() %>% round(4)
  tl[['CV_R^2']] <- (tdf %>% dplyr::select(matches("Rsquared")))[[1]] %>% mean() %>% round(4)
  
  return(tl)
}) %>% rbindlist()


PCA_SummTable %>% kable()

#   PCA-d penalized & OLS models worse vs respective regular counterparts
# Why? we do have lot less variables w PCA 117 was the optimized PCs to use
# 35 PCs capture 99.9% of Var, 60 99.99%  - other 182-239 vars not really relevant
# Indeed it might be the case the purely best model is well-overfitted
# Reccommendation: Elastic_net 
#   Sheer curiosity: test also Elastic_Net_PCA

data_holdout$pred_E.Net <- predict(enet_fit,newdata = data_holdout)
Hd_RMSE <- RMSE(data_holdout$pred_E.Net,data_holdout$logTotalValue) %>% round(4)
Hd_MAE <-  MAE(data_holdout$pred_E.Net,data_holdout$logTotalValue) %>% round(4)

data_holdout2$pred_E.NetPca <- predict(enet_fit_pca,newdata = data_holdout2)
Hd_RMSE_Pc <- RMSE(data_holdout2$pred_E.NetPca,data_holdout2$logTotalValue) %>% round(4)
Hd_MAE_Pc <-  MAE(data_holdout2$pred_E.NetPca,data_holdout2$logTotalValue) %>% round(4)

HoldoutSumms <- rbind(cbind(Hd_RMSE,Hd_MAE),cbind(Hd_RMSE_Pc,Hd_MAE_Pc)) %>% as.data.frame()

rownames(HoldoutSumms) <- c("Best_Elastic_Net","Elastic_Net_PCA")
HoldoutSumms %>% kable()

```



```{r, , message = F, warning = F, echo = T}

```



# Appendices

### Raw Histograms, Box- & Scatterplots

```{r App 1 , message = F, warning = F, echo = T, fig.height=2.5, fig.width= 5}

Hists() 

```


### Transformed Box- & Scatterplots

```{r App 2 , message = F, warning = F, echo = T, fig.height=2.5, fig.width= 5}

Boxes_n_Scatters()

```
